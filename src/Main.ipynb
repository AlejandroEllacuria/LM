{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e77d8f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils.importacioneLM'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# PROYECTO ML (NIVEL MÁSTER) — HeartDisease\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Versión ampliada:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# - Reservo TEST para evaluación final (1 única vez, sin “mirarlo” para decidir)\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimportacioneLM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfuncionesLM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     16\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils.importacioneLM'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROYECTO ML (NIVEL MÁSTER) — HeartDisease\n",
    "# Versión ampliada:\n",
    "# - EDA completo + baseline defendible + benchmarking multi-modelo\n",
    "# - Optuna sobre el modelo ganador (hiperparámetros)\n",
    "# - Ajuste de umbral (threshold) para mejorar Recall\n",
    "#\n",
    "# Principios que sigo (y que justifico en la memoria):\n",
    "# - Hago split train/test ANTES de EDA (evito leakage)\n",
    "# - Tomo decisiones (EDA, hiperparámetros, umbral) usando SOLO TRAIN\n",
    "# - Reservo TEST para evaluación final (1 única vez, sin “mirarlo” para decidir)\n",
    "# ============================================================\n",
    "from utils.importacioneLM import *\n",
    "from utils.funcionesLM import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================================================\n",
    "# 0) CONFIG\n",
    "# ============================================================\n",
    "DATA_PATH = \"data/heart.csv\"\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "PRIMARY_SCORING = \"roc_auc\"  # métrica principal para selección del modelo\n",
    "SAVE_MODEL = True\n",
    "MODEL_DIR = \"model\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"modelo_final.pkl\")\n",
    "\n",
    "PLOT = True  # si lo pongo a False, acelero ejecuciones quitando gráficos\n",
    "\n",
    "# ============================================================\n",
    "# 1) ESTILO GLOBAL\n",
    "# ============================================================\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "palette = sns.color_palette(\"Reds_r\", 8)\n",
    "sns.set_palette(palette)\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 110,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10\n",
    "})\n",
    "\n",
    "def add_caption(text: str, y_offset: float = -0.06) -> None:\n",
    "    plt.figtext(0.5, y_offset, text, ha=\"center\", fontsize=9, color=\"gray\")\n",
    "\n",
    "# ============================================================\n",
    "# 2) CARGA + RENOMBRADO A ESPAÑOL\n",
    "# ============================================================\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "\n",
    "RENAME = {\n",
    "    \"Age\": \"edad\",\n",
    "    \"Sex\": \"sexo\",\n",
    "    \"ChestPainType\": \"tipo_dolor_pecho\",\n",
    "    \"RestingBP\": \"presion_reposo\",\n",
    "    \"Cholesterol\": \"colesterol\",\n",
    "    \"FastingBS\": \"glucosa_ayunas\",\n",
    "    \"RestingECG\": \"ecg_reposo\",\n",
    "    \"MaxHR\": \"fc_max\",\n",
    "    \"ExerciseAngina\": \"angina_ejercicio\",\n",
    "    \"Oldpeak\": \"oldpeak\",\n",
    "    \"ST_Slope\": \"pendiente_st\",\n",
    "    \"HeartDisease\": \"enfermedad_cardiaca\"\n",
    "}\n",
    "df = df_raw.rename(columns=RENAME).copy()\n",
    "\n",
    "TARGET = \"enfermedad_cardiaca\"\n",
    "\n",
    "LABELS = {\n",
    "    \"edad\": \"Edad del paciente (años)\",\n",
    "    \"sexo\": \"Sexo\",\n",
    "    \"tipo_dolor_pecho\": \"Tipo de dolor en el pecho\",\n",
    "    \"presion_reposo\": \"Presión arterial en reposo (mm Hg)\",\n",
    "    \"colesterol\": \"Colesterol sérico (mg/dl)\",\n",
    "    \"glucosa_ayunas\": \"Glucosa en ayunas > 120 mg/dl\",\n",
    "    \"ecg_reposo\": \"Electrocardiograma en reposo\",\n",
    "    \"fc_max\": \"Frecuencia cardíaca máxima alcanzada\",\n",
    "    \"angina_ejercicio\": \"Angina inducida por ejercicio\",\n",
    "    \"oldpeak\": \"Depresión del segmento ST (oldpeak)\",\n",
    "    \"pendiente_st\": \"Pendiente del segmento ST\",\n",
    "    \"enfermedad_cardiaca\": \"Enfermedad cardíaca\",\n",
    "}\n",
    "\n",
    "def label(col: str) -> str:\n",
    "    return LABELS.get(col, col)\n",
    "\n",
    "CATEGORY_VALUES = {\n",
    "    \"sexo\": {\"M\": \"Hombre\", \"F\": \"Mujer\"},\n",
    "    \"angina_ejercicio\": {\"Y\": \"Sí\", \"N\": \"No\"},\n",
    "    \"tipo_dolor_pecho\": {\n",
    "        \"ASY\": \"Asintomático\",\n",
    "        \"ATA\": \"Angina atípica\",\n",
    "        \"NAP\": \"Dolor no anginoso\",\n",
    "        \"TA\": \"Angina típica\"\n",
    "    },\n",
    "    \"ecg_reposo\": {\n",
    "        \"Normal\": \"Normal\",\n",
    "        \"ST\": \"Alteraciones ST-T\",\n",
    "        \"LVH\": \"Hipertrofia ventricular izquierda\"\n",
    "    },\n",
    "    \"pendiente_st\": {\n",
    "        \"Up\": \"Pendiente ascendente\",\n",
    "        \"Flat\": \"Pendiente plana\",\n",
    "        \"Down\": \"Pendiente descendente\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def translate_levels_for_plot(df_plot: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    \"\"\"Traduce niveles categóricos SOLO para visualización.\"\"\"\n",
    "    out = df_plot.copy()\n",
    "    if col in CATEGORY_VALUES:\n",
    "        out[col] = out[col].map(CATEGORY_VALUES[col]).fillna(out[col])\n",
    "    return out\n",
    "\n",
    "# ============================================================\n",
    "# 3) HOLD-OUT SPLIT (antes de EDA para evitar leakage)\n",
    "# ============================================================\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 4) TIPOS DE VARIABLES\n",
    "# ============================================================\n",
    "categorical_cols = [\"sexo\", \"tipo_dolor_pecho\", \"ecg_reposo\", \"angina_ejercicio\", \"pendiente_st\"]\n",
    "binary_cols = [\"glucosa_ayunas\"]\n",
    "numeric_cols = [\"edad\", \"presion_reposo\", \"colesterol\", \"fc_max\", \"oldpeak\"]\n",
    "ALL_BASE_COLS = set(categorical_cols + binary_cols + numeric_cols)\n",
    "\n",
    "def nice_feature_name(feat: str) -> str:\n",
    "    \"\"\"\n",
    "    Formatea nombres de features para plots:\n",
    "    - original: 'fc_max' -> etiqueta humana\n",
    "    - one-hot: 'pendiente_st_Up' -> 'Pendiente del segmento ST = Pendiente ascendente'\n",
    "    \"\"\"\n",
    "    if feat in ALL_BASE_COLS:\n",
    "        return label(feat)\n",
    "\n",
    "    matched_base = None\n",
    "    for base in categorical_cols:\n",
    "        prefix = base + \"_\"\n",
    "        if feat.startswith(prefix):\n",
    "            if matched_base is None or len(base) > len(matched_base):\n",
    "                matched_base = base\n",
    "\n",
    "    if matched_base is not None:\n",
    "        val = feat[len(matched_base) + 1:]\n",
    "        val_label = CATEGORY_VALUES.get(matched_base, {}).get(val, val)\n",
    "        return f\"{label(matched_base)} = {val_label}\"\n",
    "\n",
    "    return feat\n",
    "\n",
    "# ============================================================\n",
    "# 5) EDA (solo TRAIN)\n",
    "# ============================================================\n",
    "def eda_train(Xtr: pd.DataFrame, ytr: pd.Series) -> None:\n",
    "    # 5.1 Distribución del target\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.countplot(x=ytr, palette=palette)\n",
    "        plt.title(\"Distribución del diagnóstico (train)\")\n",
    "        plt.xlabel(label(TARGET))\n",
    "        plt.ylabel(\"Número de pacientes\")\n",
    "        plt.xticks([0, 1], [\"No\", \"Sí\"])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.2 Correlación numéricas + target (trato ceros sospechosos como NaN solo para EDA)\n",
    "    num_df = pd.concat([Xtr[numeric_cols + binary_cols], ytr.rename(TARGET)], axis=1).copy()\n",
    "    for c in [\"presion_reposo\", \"colesterol\", \"fc_max\"]:\n",
    "        if c in num_df.columns:\n",
    "            num_df[c] = num_df[c].replace(0, np.nan)\n",
    "\n",
    "    corr_num = num_df.corr(numeric_only=True)\n",
    "\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(12, 9))\n",
    "        sns.heatmap(corr_num, annot=True, fmt=\".2f\", linewidths=0.5, square=True)\n",
    "        plt.title(\"Matriz de correlación (train: variables numéricas + target)\")\n",
    "        add_caption(\"Uso train para evitar leakage. Algunos ceros sospechosos se tratan como NaN (solo EDA).\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.3 Ranking |corr| con el target (numéricas)\n",
    "    corr_target_num = (\n",
    "        corr_num[TARGET]\n",
    "        .drop(labels=[TARGET])\n",
    "        .abs()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(\n",
    "            x=corr_target_num.values,\n",
    "            y=[label(c) for c in corr_target_num.index],\n",
    "            palette=palette\n",
    "        )\n",
    "        plt.title(\"Correlación absoluta con el target (train: solo numéricas)\")\n",
    "        plt.xlabel(\"|corr|\")\n",
    "        plt.ylabel(\"Variable\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.4 Boxplots por clase\n",
    "    train_df = Xtr.copy()\n",
    "    train_df[TARGET] = ytr\n",
    "\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=TARGET, y=\"fc_max\", data=train_df)\n",
    "        plt.title(\"Frecuencia cardíaca máxima según diagnóstico (train)\")\n",
    "        plt.xlabel(label(TARGET))\n",
    "        plt.ylabel(label(\"fc_max\"))\n",
    "        plt.xticks([0, 1], [\"No\", \"Sí\"])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        sns.boxplot(x=TARGET, y=\"oldpeak\", data=train_df)\n",
    "        plt.title(\"Oldpeak según diagnóstico (train)\")\n",
    "        plt.xlabel(label(TARGET))\n",
    "        plt.ylabel(label(\"oldpeak\"))\n",
    "        plt.xticks([0, 1], [\"No\", \"Sí\"])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.5 Chi² para categóricas (solo asociación categórica vs target)\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    X_cat_train = ohe.fit_transform(Xtr[categorical_cols])\n",
    "    cat_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "\n",
    "    chi2_scores, chi2_p = chi2(X_cat_train, ytr)\n",
    "\n",
    "    chi2_df = pd.DataFrame({\n",
    "        \"feature\": cat_feature_names,\n",
    "        \"chi2\": chi2_scores,\n",
    "        \"p_value\": chi2_p\n",
    "    }).sort_values(\"chi2\", ascending=False)\n",
    "\n",
    "    top_k_cat = 15\n",
    "    chi2_plot = chi2_df.head(top_k_cat).copy()\n",
    "    chi2_plot[\"feature_nice\"] = chi2_plot[\"feature\"].apply(nice_feature_name)\n",
    "\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=chi2_plot, x=\"chi2\", y=\"feature_nice\", palette=palette)\n",
    "        plt.title(f\"Chi² (train): Top {top_k_cat} categorías más asociadas al target\")\n",
    "        plt.xlabel(\"Estadístico Chi² (mayor = más asociación)\")\n",
    "        plt.ylabel(\"Categoría\")\n",
    "        add_caption(\"Chi² es adecuado para categóricas vs target.\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.6 Mutual Information (mixto)\n",
    "    X_mi = Xtr.copy()\n",
    "    for c in numeric_cols:\n",
    "        X_mi[c] = pd.to_numeric(X_mi[c], errors=\"coerce\")\n",
    "        if c in [\"presion_reposo\", \"colesterol\", \"fc_max\"]:\n",
    "            X_mi[c] = X_mi[c].replace(0, np.nan)\n",
    "        X_mi[c] = X_mi[c].fillna(X_mi[c].median())\n",
    "\n",
    "    pre_mi = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", \"passthrough\", numeric_cols + binary_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "    X_pre = pre_mi.fit_transform(X_mi)\n",
    "\n",
    "    cat_names = list(pre_mi.named_transformers_[\"cat\"].get_feature_names_out(categorical_cols))\n",
    "    feature_names = (numeric_cols + binary_cols) + cat_names\n",
    "\n",
    "    mi = mutual_info_classif(X_pre, ytr, random_state=RANDOM_STATE)\n",
    "    mi_df = pd.DataFrame({\"feature\": feature_names, \"mi\": mi}).sort_values(\"mi\", ascending=False)\n",
    "\n",
    "    top_k_mi = 20\n",
    "    mi_plot = mi_df.head(top_k_mi).copy()\n",
    "    mi_plot[\"feature_nice\"] = mi_plot[\"feature\"].apply(nice_feature_name)\n",
    "\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(data=mi_plot, x=\"mi\", y=\"feature_nice\", palette=palette)\n",
    "        plt.title(f\"Mutual Information (train): Top {top_k_mi} variables más informativas\")\n",
    "        plt.xlabel(\"MI (mayor = más información sobre el target)\")\n",
    "        plt.ylabel(\"Variable\")\n",
    "        add_caption(\"MI captura relaciones no lineales y sirve para variables mixtas.\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.7 VIF multicolinealidad (numéricas)\n",
    "    X_vif = Xtr[numeric_cols].copy()\n",
    "    for c in X_vif.columns:\n",
    "        X_vif[c] = pd.to_numeric(X_vif[c], errors=\"coerce\")\n",
    "        if c in [\"presion_reposo\", \"colesterol\", \"fc_max\"]:\n",
    "            X_vif[c] = X_vif[c].replace(0, np.nan)\n",
    "        X_vif[c] = X_vif[c].fillna(X_vif[c].median())\n",
    "\n",
    "    X_vif_const = sm.add_constant(X_vif)\n",
    "\n",
    "    vif_rows = []\n",
    "    for i, col in enumerate(X_vif_const.columns):\n",
    "        if col == \"const\":\n",
    "            continue\n",
    "        vif_rows.append({\"variable\": col, \"VIF\": variance_inflation_factor(X_vif_const.values, i)})\n",
    "\n",
    "    vif_df = pd.DataFrame(vif_rows).sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "    if PLOT:\n",
    "        plt.figure(figsize=(9, 4.8))\n",
    "        sns.barplot(data=vif_df, x=\"VIF\", y=\"variable\", palette=palette)\n",
    "        plt.title(\"VIF (train): multicolinealidad en variables numéricas\")\n",
    "        plt.xlabel(\"VIF (mayor = más redundancia)\")\n",
    "        plt.ylabel(\"Variable\")\n",
    "        plt.yticks(\n",
    "            ticks=range(len(vif_df[\"variable\"])),\n",
    "            labels=[label(v) for v in vif_df[\"variable\"]]\n",
    "        )\n",
    "        add_caption(\"Si VIF es alto, puedo considerar regularización o eliminar redundancias.\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # 5.8 Tasa media del target por categoría\n",
    "    if PLOT:\n",
    "        for col in categorical_cols:\n",
    "            tmp = pd.DataFrame({col: Xtr[col], TARGET: ytr}).dropna()\n",
    "            tmp = translate_levels_for_plot(tmp, col)\n",
    "\n",
    "            rate = tmp.groupby(col)[TARGET].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "            plt.figure(figsize=(9, 5))\n",
    "            sns.barplot(data=rate, x=TARGET, y=col, palette=palette)\n",
    "            plt.title(f\"Tasa media de enfermedad por {label(col)} (train)\")\n",
    "            plt.xlabel(\"Probabilidad media de enfermedad\")\n",
    "            plt.ylabel(label(col))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Ejecutar EDA (solo train)\n",
    "eda_train(X_train, y_train)\n",
    "\n",
    "# ============================================================\n",
    "# 6) PREPROCESADO (pipelines)\n",
    "# ============================================================\n",
    "# Nota: aquí imputo NaN. En este dataset hay ceros no fisiológicos en algunas columnas,\n",
    "# pero para mantener el ejemplo simple y reproducible, sigo tu imputación. Si quieres,\n",
    "# puedo añadir un paso que convierta 0->NaN en columnas concretas ANTES de imputar.\n",
    "numeric_imputer = SimpleImputer(strategy=\"median\")\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "preprocess_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", numeric_imputer),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), numeric_cols + binary_cols),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", cat_imputer),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "preprocess_noscale = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline(steps=[\n",
    "            (\"imputer\", numeric_imputer)\n",
    "        ]), numeric_cols + binary_cols),\n",
    "        (\"cat\", Pipeline(steps=[\n",
    "            (\"imputer\", cat_imputer),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# ============================================================\n",
    "# 7) BASELINE DEFENDIBLE — LogReg balanced + CV\n",
    "# ============================================================\n",
    "baseline_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "                (\"scaler\", StandardScaler())\n",
    "            ]), numeric_cols),\n",
    "            (\"bin\", SimpleImputer(strategy=\"most_frequent\"), binary_cols),\n",
    "            (\"cat\", Pipeline(steps=[\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "            ]), categorical_cols),\n",
    "        ],\n",
    "        remainder=\"drop\"\n",
    "    )),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",  # yo lo pongo para ayudar al recall si hay desbalance\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "scoring = {\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"avg_precision\": \"average_precision\",\n",
    "    \"recall_pos\": \"recall\",\n",
    "    \"precision_pos\": \"precision\",\n",
    "    \"f1_pos\": \"f1\"\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    baseline_pipe,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=cv,\n",
    "    scoring=scoring,\n",
    "    n_jobs=-1,\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "cv_summary = {k: (np.mean(v), np.std(v)) for k, v in cv_results.items() if k.startswith(\"test_\")}\n",
    "\n",
    "print(\"\\n=== Baseline (LogReg balanced) — CV en TRAIN (media ± std) ===\")\n",
    "for k, (m, s) in cv_summary.items():\n",
    "    print(f\"{k.replace('test_','')}: {m:.3f} ± {s:.3f}\")\n",
    "\n",
    "# ============================================================\n",
    "# 8) BENCHMARK MULTI-MODELO — GridSearchCV (rápido y defendible)\n",
    "# ============================================================\n",
    "def evaluate_on_test(model, Xte, yte) -> dict:\n",
    "    # Yo evalúo con predicción dura (0/1) y con probabilidades para AUC.\n",
    "    y_pred = model.predict(Xte)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(Xte)[:, 1]\n",
    "    else:\n",
    "        # Si el modelo no da probas, normalizo decision_function a [0,1]\n",
    "        scores = model.decision_function(Xte)\n",
    "        y_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(yte, y_pred),\n",
    "        \"precision\": precision_score(yte, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(yte, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(yte, y_pred, zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(yte, y_proba),\n",
    "        \"pr_auc\": average_precision_score(yte, y_proba),\n",
    "    }\n",
    "\n",
    "# XGBoost opcional\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append({\n",
    "    \"name\": \"LogisticRegression\",\n",
    "    \"pipeline\": Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess_scaled),\n",
    "        (\"model\", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"param_grid\": {\n",
    "        \"model__C\": [0.01, 0.1, 1, 10],\n",
    "        \"model__penalty\": [\"l2\"],\n",
    "        \"model__solver\": [\"lbfgs\"]\n",
    "    },\n",
    "    \"why\": \"Baseline interpretable y defendible.\"\n",
    "})\n",
    "\n",
    "models.append({\n",
    "    \"name\": \"SVC\",\n",
    "    \"pipeline\": Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess_scaled),\n",
    "        (\"model\", SVC(probability=True, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"param_grid\": {\n",
    "        \"model__C\": [0.1, 1, 10],\n",
    "        \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "        \"model__gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    \"why\": \"Capta fronteras complejas; requiere escalado.\"\n",
    "})\n",
    "\n",
    "models.append({\n",
    "    \"name\": \"KNN\",\n",
    "    \"pipeline\": Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess_scaled),\n",
    "        (\"model\", KNeighborsClassifier())\n",
    "    ]),\n",
    "    \"param_grid\": {\n",
    "        \"model__n_neighbors\": [5, 11, 21, 31],\n",
    "        \"model__weights\": [\"uniform\", \"distance\"],\n",
    "        \"model__p\": [1, 2]\n",
    "    },\n",
    "    \"why\": \"Baseline no paramétrico; sensible al escalado.\"\n",
    "})\n",
    "\n",
    "models.append({\n",
    "    \"name\": \"RandomForest\",\n",
    "    \"pipeline\": Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess_noscale),\n",
    "        (\"model\", RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"param_grid\": {\n",
    "        \"model__n_estimators\": [200, 500],\n",
    "        \"model__max_depth\": [None, 3, 5, 8],\n",
    "        \"model__min_samples_split\": [2, 5, 10],\n",
    "        \"model__min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"why\": \"Robusto en tabular; no necesita escalado.\"\n",
    "})\n",
    "\n",
    "models.append({\n",
    "    \"name\": \"GradientBoosting\",\n",
    "    \"pipeline\": Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess_noscale),\n",
    "        (\"model\", GradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"param_grid\": {\n",
    "        \"model__n_estimators\": [200, 400],\n",
    "        \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "        \"model__max_depth\": [2, 3]\n",
    "    },\n",
    "    \"why\": \"Boosting suele capturar interacciones y no linealidad.\"\n",
    "})\n",
    "\n",
    "if HAS_XGB:\n",
    "    models.append({\n",
    "        \"name\": \"XGBoost\",\n",
    "        \"pipeline\": Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess_noscale),\n",
    "            (\"model\", XGBClassifier(\n",
    "                random_state=RANDOM_STATE,\n",
    "                eval_metric=\"logloss\",\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            \"model__n_estimators\": [300, 600],\n",
    "            \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"model__max_depth\": [2, 3, 4],\n",
    "            \"model__subsample\": [0.8, 1.0],\n",
    "            \"model__colsample_bytree\": [0.8, 1.0],\n",
    "            \"model__reg_lambda\": [1.0, 5.0, 10.0]\n",
    "        },\n",
    "        \"why\": \"Candidato final productivo en tabular; regularizado.\"\n",
    "    })\n",
    "else:\n",
    "    models.append({\n",
    "        \"name\": \"HistGradientBoosting\",\n",
    "        \"pipeline\": Pipeline(steps=[\n",
    "            (\"preprocess\", preprocess_noscale),\n",
    "            (\"model\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            \"model__learning_rate\": [0.03, 0.05, 0.1],\n",
    "            \"model__max_depth\": [3, 5, None],\n",
    "            \"model__max_iter\": [300, 600]\n",
    "        },\n",
    "        \"why\": \"Boosting moderno en sklearn si XGBoost no está.\"\n",
    "    })\n",
    "\n",
    "results = []\n",
    "best_estimators = {}\n",
    "\n",
    "print(\"\\n=== GridSearchCV por modelo (TRAIN CV) + evaluación en TEST (solo comparativa) ===\")\n",
    "for spec in models:\n",
    "    # Yo uso CV en TRAIN para elegir hiperparámetros del modelo\n",
    "    gs = GridSearchCV(\n",
    "        estimator=spec[\"pipeline\"],\n",
    "        param_grid=spec[\"param_grid\"],\n",
    "        scoring=PRIMARY_SCORING,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        refit=True\n",
    "    )\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    best_model = gs.best_estimator_\n",
    "    best_estimators[spec[\"name\"]] = best_model\n",
    "\n",
    "    # Nota metodológica:\n",
    "    # aquí muestro métricas de TEST para comparar modelos,\n",
    "    # pero NO toco decisiones del umbral con el test.\n",
    "    test_metrics = evaluate_on_test(best_model, X_test, y_test)\n",
    "\n",
    "    results.append({\n",
    "        \"modelo\": spec[\"name\"],\n",
    "        \"cv_best_roc_auc\": gs.best_score_,\n",
    "        \"test_accuracy\": test_metrics[\"accuracy\"],\n",
    "        \"test_precision\": test_metrics[\"precision\"],\n",
    "        \"test_recall\": test_metrics[\"recall\"],\n",
    "        \"test_f1\": test_metrics[\"f1\"],\n",
    "        \"test_roc_auc\": test_metrics[\"roc_auc\"],\n",
    "        \"test_pr_auc\": test_metrics[\"pr_auc\"],\n",
    "        \"me_justifico\": spec[\"why\"]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"test_roc_auc\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Ranking (ordenado por ROC-AUC en TEST) ===\")\n",
    "print(results_df[[\"modelo\", \"cv_best_roc_auc\", \"test_roc_auc\", \"test_pr_auc\", \"test_f1\"]].to_string(index=False))\n",
    "\n",
    "if PLOT:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=results_df, x=\"test_roc_auc\", y=\"modelo\", palette=palette)\n",
    "    plt.title(\"Comparativa de modelos (test) — ROC-AUC\")\n",
    "    plt.xlabel(\"ROC-AUC (test)\")\n",
    "    plt.ylabel(\"Modelo\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=results_df, x=\"test_f1\", y=\"modelo\", palette=palette)\n",
    "    plt.title(\"Comparativa de modelos (test) — F1-score\")\n",
    "    plt.xlabel(\"F1 (test)\")\n",
    "    plt.ylabel(\"Modelo\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 9) OPTUNA — hiperparámetros finos sobre el modelo ganador\n",
    "# ============================================================\n",
    "# Aquí hago exactamente lo que me pide el profesor:\n",
    "# uso Optuna para mejorar hiperparámetros \"sobre lo que ya tengo\".\n",
    "best_name = results_df.iloc[0][\"modelo\"]\n",
    "print(f\"\\n>>> Modelo ganador (por ROC-AUC en test, comparativa): {best_name}\")\n",
    "\n",
    "def build_optuna_pipeline(model_name: str, params: dict) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Yo clono el pipeline ganador y le inyecto parámetros al estimador.\n",
    "    Optuna devuelve keys sin 'model__', así que aquí las convierto.\n",
    "    \"\"\"\n",
    "    pipe = clone(best_estimators[model_name])\n",
    "    pipe.set_params(**{f\"model__{k}\": v for k, v in params.items()})\n",
    "    return pipe\n",
    "\n",
    "def objective(trial):\n",
    "    # Defino el espacio de búsqueda en función del modelo ganador.\n",
    "    # Esto evita buscar parámetros que no existen.\n",
    "    if best_name == \"LogisticRegression\":\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-3, 1e2, log=True),\n",
    "            \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "        }\n",
    "\n",
    "    elif best_name == \"SVC\":\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 1e-2, 1e2, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"rbf\", \"linear\"]),\n",
    "            \"gamma\": trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"]),\n",
    "        }\n",
    "\n",
    "    elif best_name == \"KNN\":\n",
    "        params = {\n",
    "            \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 3, 45),\n",
    "            \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "            \"p\": trial.suggest_categorical(\"p\", [1, 2]),\n",
    "        }\n",
    "\n",
    "    elif best_name == \"RandomForest\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1200),\n",
    "            \"max_depth\": trial.suggest_categorical(\"max_depth\", [None, 3, 5, 8, 12]),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 30),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 15),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None]),\n",
    "            \"class_weight\": trial.suggest_categorical(\"class_weight\", [None, \"balanced\"]),\n",
    "        }\n",
    "\n",
    "    elif best_name in [\"GradientBoosting\", \"HistGradientBoosting\"]:\n",
    "        params = {\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            \"max_depth\": trial.suggest_categorical(\"max_depth\", [2, 3, 4, 5, None]),\n",
    "        }\n",
    "        if best_name == \"HistGradientBoosting\":\n",
    "            params[\"max_iter\"] = trial.suggest_int(\"max_iter\", 200, 1200)\n",
    "\n",
    "    elif best_name == \"XGBoost\":\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1200),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1.0, 20.0),\n",
    "        }\n",
    "    else:\n",
    "        params = {}\n",
    "\n",
    "    pipe = build_optuna_pipeline(best_name, params)\n",
    "\n",
    "    # Yo optimizo roc_auc con CV estratificada en TRAIN\n",
    "    score = cross_val_score(\n",
    "        pipe, X_train, y_train,\n",
    "        cv=cv,\n",
    "        scoring=PRIMARY_SCORING,\n",
    "        n_jobs=-1\n",
    "    ).mean()\n",
    "\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print(\"\\n=== Optuna (CV en TRAIN) sobre el modelo ganador ===\")\n",
    "print(\"Best CV score:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "# Con los mejores hiperparámetros, yo refiteo el pipeline en TODO TRAIN\n",
    "final_model = build_optuna_pipeline(best_name, study.best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# ============================================================\n",
    "# 10) AJUSTE DE UMBRAL (threshold) PARA MEJORAR RECALL\n",
    "# ============================================================\n",
    "# Importante:\n",
    "# - NO elijo el umbral mirando el TEST.\n",
    "# - Yo creo una validación interna dentro de TRAIN para escoger threshold.\n",
    "# - Luego fijo ese threshold y SOLO ENTONCES evalúo en TEST.\n",
    "\n",
    "X_fit, X_val, y_fit, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.25,\n",
    "    stratify=y_train,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Re-entreno en X_fit porque estoy simulando el proceso real:\n",
    "# entreno -> escojo umbral en validación -> evalúo en test.\n",
    "final_model.fit(X_fit, y_fit)\n",
    "\n",
    "# Probabilidades para clase positiva en validación\n",
    "if hasattr(final_model, \"predict_proba\"):\n",
    "    proba_val = final_model.predict_proba(X_val)[:, 1]\n",
    "else:\n",
    "    scores = final_model.decision_function(X_val)\n",
    "    proba_val = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 181)\n",
    "\n",
    "rows = []\n",
    "for t in thresholds:\n",
    "    y_pred_t = (proba_val >= t).astype(int)\n",
    "    prec = precision_score(y_val, y_pred_t, zero_division=0)\n",
    "    rec  = recall_score(y_val, y_pred_t, zero_division=0)\n",
    "    rows.append((t, prec, rec))\n",
    "\n",
    "thr_df = pd.DataFrame(rows, columns=[\"threshold\", \"precision\", \"recall\"])\n",
    "\n",
    "# Regla que uso (muy en línea con el comentario del profesor):\n",
    "# \"subo Recall sin afectar mucho Precision\".\n",
    "# Concretamente: permito caer como máximo 0.02 respecto a precision@0.5.\n",
    "t0 = 0.5\n",
    "prec0 = thr_df.loc[(thr_df[\"threshold\"] - t0).abs().idxmin(), \"precision\"]\n",
    "min_prec = max(0.0, prec0 - 0.02)\n",
    "\n",
    "candidates = thr_df[thr_df[\"precision\"] >= min_prec]\n",
    "if len(candidates) > 0:\n",
    "    best_row = candidates.sort_values(\"recall\", ascending=False).iloc[0]\n",
    "else:\n",
    "    best_row = thr_df.sort_values(\"recall\", ascending=False).iloc[0]\n",
    "\n",
    "best_threshold = float(best_row[\"threshold\"])\n",
    "\n",
    "print(\"\\n=== Selección de umbral (VALIDACIÓN dentro de TRAIN) ===\")\n",
    "print(f\"Precision@0.5 = {prec0:.3f} | min_precision = {min_prec:.3f}\")\n",
    "print(f\"Best threshold = {best_threshold:.3f} | Precision = {best_row['precision']:.3f} | Recall = {best_row['recall']:.3f}\")\n",
    "\n",
    "if PLOT:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"precision\"], label=\"Precision\")\n",
    "    plt.plot(thr_df[\"threshold\"], thr_df[\"recall\"], label=\"Recall\")\n",
    "    plt.axvline(0.5, linestyle=\"--\")\n",
    "    plt.axvline(best_threshold, linestyle=\"--\")\n",
    "    plt.title(\"Precision y Recall vs Threshold (validación en TRAIN)\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Métrica\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 11) EVALUACIÓN FINAL EN TEST (con threshold fijado)\n",
    "# ============================================================\n",
    "# Ahora sí: entreno con TODO TRAIN (ya que el threshold está decidido)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Calculo probabilidades en TEST\n",
    "if hasattr(final_model, \"predict_proba\"):\n",
    "    y_proba_final = final_model.predict_proba(X_test)[:, 1]\n",
    "else:\n",
    "    scores = final_model.decision_function(X_test)\n",
    "    y_proba_final = (scores - scores.min()) / (scores.max() - scores.min() + 1e-12)\n",
    "\n",
    "# Predicción final usando el UMBRAL elegido\n",
    "y_pred_final = (y_proba_final >= best_threshold).astype(int)\n",
    "\n",
    "roc_final = roc_auc_score(y_test, y_proba_final)\n",
    "ap_final = average_precision_score(y_test, y_proba_final)\n",
    "\n",
    "# Specificity (TNR) me viene bien en salud para complementar Recall (sensibilidad)\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "specificity = tn / (tn + fp + 1e-12)\n",
    "\n",
    "print(\"\\n=== Evaluación FINAL en TEST (umbral ajustado) ===\")\n",
    "print(f\"Modelo final (Optuna): {best_name}\")\n",
    "print(f\"Threshold final: {best_threshold:.3f}\")\n",
    "print(f\"ROC-AUC (test): {roc_final:.3f}\")\n",
    "print(f\"PR-AUC / Average Precision (test): {ap_final:.3f}\")\n",
    "print(f\"Specificity (TNR) (test): {specificity:.3f}\\n\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=[\"No\", \"Sí\"]))\n",
    "\n",
    "if PLOT:\n",
    "    plt.figure(figsize=(6.5, 4.8))\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, y_pred_final,\n",
    "        display_labels=[\"No\", \"Sí\"],\n",
    "        values_format=\"d\",\n",
    "        cmap=\"Reds\"\n",
    "    )\n",
    "    plt.title(f\"Matriz de confusión (test) — {best_name} (thr={best_threshold:.2f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    RocCurveDisplay.from_predictions(y_test, y_proba_final)\n",
    "    plt.title(f\"Curva ROC (test) — {best_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    PrecisionRecallDisplay.from_predictions(y_test, y_proba_final)\n",
    "    plt.title(f\"Curva Precision–Recall (test) — {best_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 12) GUARDADO DEL MODELO FINAL\n",
    "# ============================================================\n",
    "# Yo guardo el pipeline entero (preprocess + modelo) para reproducibilidad.\n",
    "if SAVE_MODEL:\n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    with open(MODEL_PATH, \"wb\") as f:\n",
    "        pickle.dump(final_model, f)\n",
    "    print(f\"\\n✅ Modelo final guardado en: {MODEL_PATH}\")\n",
    "\n",
    "# ============================================================\n",
    "# 13) TABLA RESUMEN (lista para memoria)\n",
    "# ============================================================\n",
    "results_df_display = results_df.copy()\n",
    "results_df_display[\"me_justifico\"] = results_df_display[\"me_justifico\"].str.slice(0, 120) + \"...\"\n",
    "print(\"\\n=== Tabla resumen de modelos (recortada para memoria) ===\")\n",
    "print(results_df_display[[\"modelo\",\"cv_best_roc_auc\",\"test_roc_auc\",\"test_pr_auc\",\"test_f1\",\"test_recall\",\"test_precision\",\"me_justifico\"]]\n",
    "      .sort_values(\"test_roc_auc\", ascending=False)\n",
    "      .to_string(index=False))\n",
    "\n",
    "print(\"\\n========================================== FIN ==========================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
